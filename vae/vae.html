<!--
  Copyright 2018 The Distill Template Authors
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at
       http://www.apache.org/licenses/LICENSE-2.0
  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
<!doctype html>

<head>
  <script src="https://distill.pub/template.v2.js"></script>
  <meta name="viewport" content="width=device-width, initial-scale=1" >
  <meta charset="utf8">
  <style>
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
</style>
</head>

<body>
<d-front-matter>
  <script id='distill-front-matter' type="text/json">{
    "title": "Variational Autoencoder review",
    "description": "Although \" extremely useful for visualizing high-dimensional data, t-SNE plots can sometimes be mysterious or misleading.",
    "published": "May 8, 2018",
    "authors": [
      {
        "author":"Jerrick Hoang",
        "authorURL":"https://jerrickhoang.github.io/",
        "affiliation":"Uber ATG",
        "affiliationURL":"https://g.co/brain"
      }
    ],
    "katex": {
      "delimiters": [
        {"left": "$$", "right": "$$", "display": false}
      ]
    }
  }</script>
</d-front-matter>
<d-title>
  <figure style="grid-column: page; margin: 1rem 0;"><img src="vae_4.png" style="width:100%; border: 1px solid rgba(0, 0, 0, 0.2);"/></figure>
  <p>The following article summarizes and reimplements the paper by Kingma and Welling<d-cite key="mercier2011humans"></d-cite>. The paper combines the traditional variational Bayes with neural networks. In particular, instead of the intractably approach by tradiational variational Bayes, the paper proposes an approach to estimate the variational lower bound using neural networks. In order for gradient descent to work, the paper introduce the <b>reparameterization trick</b> which pushes the stochastic operation of sampling out to the input, thus enabling the gradient to flow through the networks.</p>
</d-title>
<d-article>
  <a class="marker" href="#section-1" id="section-1"><span>1</span></a>
  <h2>Theoretical derivation</h2>

  <p>Imagine you want to write a digit from 0-9 on a piece of paper. You first pick a pen, pick a digit then write it. The appearance of the digit on the paper depends on the kind of pen, the color of the ink, what digit you pick, etc. In generative models, this information is called latent variable. </p>

  <p>We can state our generative problem as follows. Given a dataset <d-math>D = {x^{(i)}}^N_{i=1}</d-math> are <d-math>N</d-math> i.i.d samples of a random variable <d-math>X</d-math> drawn from a hidden distribution. We further assume there is a latent variable <d-math>Z</d-math> and the hidden distribution is such that <d-math>P(X) = \int P(X|z)P(Z) dz</d-math>. The problem is to draw more samples from this hidden distribution.</p>

  <p>However, different tasks require a completely different structure of the latent space <d-math>P(Z)</d-math>. Generating a digit requires different latent information than generating a song. One interesting hypothesis is that for any latent distribution, there exists a deterministic transformation that maps a normal distribution to the latent one.</p>

  <figure><img src="normal.png" /></figure>

  <p>Based on this insight, we can somewhat safely start with $$P(Z) = \mathcal{N}(0, I)$$. Plugging this back to the first equation gives, $$P(X) = \int P(X|z\sim\mathcal{N}(0, I))\mathcal{N}(0, I) dz$$. One approach to solve this integral is using the Monte Carlo approximation, $$ P(X) = \frac{1}{n}\sum_i P(X|Z=z_i) $$ However, for high dimensional problem, n must be extremely larg for this approximationto be close to ground truth. The key idea in variational method is that for the most part $$P(X|Z)$$ will be nearly zero. So, we want to look for the "important" region in the space of $$Z$$ by having another model $$Q_{\theta}(Z|X)$$ proposing the values of z that are likely to produce $$X$$, </p>

  <div class="l-page-outset">
  <d-math block>
     D_{KL}(Q_{\theta}(Z|X) || P(Z|X)) = E_{z\sim Q_{\theta}}[logQ_{\theta}(Z|X) - logP(Z|X)] 
     = E_{z\sim Q_{\theta}}[logQ_{\theta}(Z|X) - log\frac{P(X|Z)P(Z)}{P(X)}]
  </d-math>
  
   <d-math block>
     = E_{z\sim Q_{\theta}}[logQ_{\theta}(Z|X) - logP(X|Z) - logP(Z) + logP(X)]
   </d-math>

   <br>

   <d-math block>
     \rightarrow logP(X) - \underbrace{D_{KL}(Q_{\theta}(Z|X) || P(Z|X))}_{\geq 0} = E_{z\sim Q_{\theta}}[logP(X|Z)]
      - E_{z\sim Q_{\theta}}(logQ_{\theta}(Z|X) - logP(Z))
   </d-math>

   <d-math block>
     = \underbrace{E_{z\sim Q_{\theta}}[logP(X|Z)] - D_{KL}(Q_{\theta}(Z|X) || P(Z))}_{\mathcal{L}(Q_{\theta}, Z, X)}
   </d-math>
  </div>

  <p>Therefore, $$\mathcal{L}(Q_{\theta}, Z, X)$$ is the evidence lower bound (ELBO) of the marginal likelihood. Note, the variatonal lower bound can only be as good as our estimation of $$P(Z|X)$$ using $$Q_{\theta}(Z|X)$$. In summary, to estimate the <b>RHS</b>, we first need to estimate $$Q_{\theta}(Z|X)$$, sample $$z \sim Q_{\theta}(Z|X)$$, then estimate $$P(X|Z = z)$$. The first part is called the encoder which encodes the input into a hidden state. The second part is called the decoder, which given the hidden state, infers the input. The two parts combined is called autoencoder, (hence the name variational autoencoder).</p>

  <p>However, in order to use learn encoder and decoder jointly using gradient based methods, all operations must be differentible. The sampling operation in the middle of the encoder and decoder is not. The paper introduces a technique called reparameterization trick which pushes the sampling operation to the input layer. It works by sampling $$\epsilon \sim \mathcal{N(0, I)}$$. If the encoder $$Q_{\theta}(Z|X)$$ produces $$\mu(X), \Sigma(X)$$, then the input to the decoder will then be $$z = \mu(X) + \Sigma^{1/2}(X)*\epsilon$$.</p>

  <figure><img src="reparam.png" style="width:100%; border: 1px solid rgba(0, 0, 0, 0.2);"/></figure>
  
  <a class="marker" href="#section-2" id="section-2"><span>2</span></a>
  <h2>Implementation</h2>

  <p>Let's look at an example using MNIST dataset. </p>
  <p>The encoder takes the 28x28 and produces a n-dimensional mean and variance for z (to ensure variance is positive we predict log $\Sigma$ instead, then $$e^{log\Sigma}$$ is positive )</p>
  <d-code block language="python">
    with tf.variable_scope('encoder'):
      fc = slim.fully_connected(self.input_x, 512, activation_fn=tf.nn.relu)
      fc = slim.fully_connected(fc, 384, activation_fn=tf.nn.relu)
      fc = slim.fully_connected(fc, 256, activation_fn=tf.nn.relu)
      mu = slim.fully_connected(fc, 10, activation_fn=None)
      log_sigma = slim.fully_connected(fc, 10, activation_fn=None)
  </d-code>

  <p>Reparameterization trick, first sample $\epsilon \sim \mathcal{N}(0, I)$, then scale $$z = \mu(X) + \Sigma^{1/2}(X) * \epsilon$$,</p>

  <d-code block language="python">
    with tf.variable_scope('z'):
      eps = tf.random_normal(shape=tf.shape(log_sigma), mean=0, stddev=1, dtype=tf.float32)
      self.z = mu + tf.sqrt(tf.exp(log_sigma)) * eps
  </d-code>

  <p>The decoder takes $z$ then tries to reproduce input $$\hat{x}$$, </p>

  <d-code block language="python">
    with tf.variable_scope('decoder'):
      dec = slim.fully_connected(self.z, 256, activation_fn=tf.nn.relu)
      dec = slim.fully_connected(dec, 384, activation_fn=tf.nn.relu)
      dec = slim.fully_connected(dec, 512, activation_fn=tf.nn.relu)
      dec = slim.fully_connected(dec, 784, activation_fn=None)
  </d-code>

  <p>Our objective is to maximize the evidence lower bound (ELBO),</p> 
  <d-math>[ \mathcal{L}(Q_{\theta}, X, Z) = E_{x \sim D}[E_{z\sim Q_{\theta(Z|X=x)}}[logP(X|Z=z)] - D_{KL}(Q_{\theta}(Z|X=x) || P(Z))] ] </d-math>
  <p>So,</p> 
  <d-math>
  \theta = \underset{\theta}{argmax} \mathcal{L}(Q_{\theta}, X, Z) = \underset{\theta}{argmin} - \mathcal{L}(Q_{\theta}, X, Z) = 
  </d-math>
  <d-math>
  \underset{\theta}{argmin} E_{x\sim D}[E_{z\sim Q_{\theta}(Z|X=x)}[-logP(X|Z=z)] + D_{KL}(Q_{\theta}(Z|X=x)||P(Z))] 
  </d-math>
  <d-math>= \underset{\theta}{argmin} H(D, P(X|Z)) + D_{KL}(Q_{\theta}(Z|X)||P(Z))
  </d-math>
  <p> The implementation of the cross entropy can be as simple as, </p>

  <d-code block language="python">
    self.rec = tf.reduce_mean(tf.reduce_sum(
    tf.nn.sigmoid_cross_entropy_with_logits(logits=dec, labels=self.input_x), 1))
  </d-code>

  <p>Since we assume Gaussian for $$P(Z)$$ and $$Q(Z)$$, the KL loss can be calculated in closed form,</p> 
  
  <div class="l-page-outset">
  <d-math>
  D_{KL}(Q_{\theta}(Z|X)||P(Z)) = D_{KL}(\mathcal{N}(\mu(X), \Sigma(X))||\mathcal{N}(0, I)) = \frac{1}{2}(tr(\Sigma(X)) + \mu(X)^T\mu(X) - k - \log \det(\Sigma(X)))
  </d-math>
  </div>

  <d-code block language="python">
    self.kl_loss = tf.reduce_mean(0.5 * tf.reduce_sum(tf.exp(log_sigma) + 
                   tf.square(mu) - 1. - log_sigma, 1))
  </d-code>

  <p>The total loss would just be the sum of the two losses</p>
  <d-code block language="python">
     self.loss = tf.reduce_mean(self.rec + self.kl_loss)
  </d-code>

  <a class="marker" href="#section-3" id="section-3"><span>3</span></a>
  <h2>Results and discussion</h2>

  <figure><img src="reconstruct.png" style="width:100%; border: 1px solid rgba(0, 0, 0, 0.2);"/></figure>

  <p>Here is the results of reconstruction for 100 random images after 10 minutes of training (10000 iterations, with batch size = 100, learning rate = 1e-3, dimension of z = 10). Assume the columns are 0-indexed, then the even columns contain reconstructed images and the odd columns contain real images.</p>

  <figure><img src="generated.png" style="width:100%; border: 1px solid rgba(0, 0, 0, 0.2);"/></figure>

  <p>Here are the digits generated by the same model from a gaussian latent space.</p>
  <p>One interesting observation in training VAE is that: initially, both reconstruction loss and KL loss go down. At one point, KL loss starts going up while total loss still goes down. To understand this phenomenon, notice that in a correctly trained VAE and with structured data, KL loss cannot be 0. If KL loss equals 0, it means there is no structured subspace of the prior distribution for the latent variable. Or, the data is almost completely random. At first, both $P$ and $Q$ start out to be random, so both losses go down as $P$ gets good at producing the average of the data, and $Q$ gets good at finding the subspace in the latent space. After this phase, $P$ and $Q$ start making trade-off. Specifically, if $Q$ allows more mass in the subspace, more noise will be generated by $P$ hence the reconstruction loss will go up and vice versa.</p>

  <figure><img src="train_process.png" ></figure>

  <p>To make things more interesting, we can train a different model with laten dimension = 2 and visualize it,</p>

  <figure><img src="top_infer.png" style="width:100%; border: 1px solid rgba(0, 0, 0, 0.2);"/></figure>
  <p>If we sample spatially from the 2D gaussian of the latent space and with each value $$z$$, we generate a digit, then the top down view looks something like the picture above. We can see that the latent space encodes the skewness of the digits.</p>

  <figure><img src="top_label.png" style="width:100%; border: 1px solid rgba(0, 0, 0, 0.2);"/></figure>
  <p>If we repeat the same sampling mechanism and now encode the digits as colors, we can see that from the original Gaussian, the encoder networks has learnt a non-linear transformation that transoforms the Gaussian into different subspaces encoding different digits.</p>

  <p>That concludes my summary and experiments with the original VAE, more theoretical analysis as well as experiments with recent variations to come.</p>





</d-article>

<d-appendix>
  <d-bibliography src="bibliography.bib"></d-bibliography>
</d-appendix>

<distill-footer></distill-footer>

</body>

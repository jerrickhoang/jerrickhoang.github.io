<!--
  Copyright 2018 The Distill Template Authors
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at
       http://www.apache.org/licenses/LICENSE-2.0
  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
<!doctype html>

<head>
  <script src="https://distill.pub/template.v2.js"></script>
  <meta name="viewport" content="width=device-width, initial-scale=1" >
  <meta charset="utf8">
  <style></style>
</head>

<body>
<d-front-matter>
  <script id='distill-front-matter' type="text/json">{
    "title": "Reinforcement Learning 1: The grid world",
    "published": "May 8, 2018",
    "authors": [
      {
        "author":"Jerrick Hoang",
        "authorURL":"https://jerrickhoang.github.io/",
        "affiliation":"Uber ATG",
        "affiliationURL":"https://g.co/brain"
      }
    ],
    "katex": {
      "delimiters": [
        {"left": "$$", "right": "$$", "display": false} ]
    }
  }</script>
</d-front-matter>
<d-title>
  <figure style="grid-column: page; margin: 1rem 0;"><img src="cover.png" style="width:100%; border: 1px solid rgba(0, 0, 0, 0.2);"/></figure>
  <p>In this short post, I want to give an introduction to the problem statement and characteristics of reinforcement learning as well as implement some basic techniques to solve grid world. This is chapter 1 of a series of posts about modern reinforcement learning techniques.</p>

</d-title>
<d-article>
  <a class="marker" href="#section-1" id="section-1"><span>1</span></a>
  <h2>Motivation</h2>

  <p>In <b>supervised learning</b>, a dataset (often drawn i.i.d from a hidden distribution) is provided by a supervisor. The optimization problem is to maximize a certain loss function, e.g. L2 or cross entropy. While this setup is useful in some problems, it's missing certain characteristics that are crucial to how humans learn things,</p>
  <ul>
    <li>There is no supervisor</li>
    <li>The feedback from the environment is delayed, not instantaneous </li>
    <li>The experience is sequential, not i.i.d</li>
    <li>Agent's actions affect the subsequent data it receives</li>
  </ul>

  <p>Reinforcement learning tries to address this by stating the problem in a slightly different way. In state of having a stateless, i.i.d dataset, assuming there's an agent with a set of actions $$\mathcal{A}$$. The agent starts at state $$s_0$$, At each timestep $$t$$, the agent pick an action $$a_t$$. Upon receiving the action $$a_t$$ from the agent, the environment returns an observation $$o_{t+1}$$ and the reward $$r_t$$. Upon receiving the reward $$r_t$$ and observation $$o_{t+1}$$, the agent ended up at a new state $$s_{t+1}$$, and the process keeps repeating. The objective of the agent is to maximize the cumulative reward $$g_t(s_0) = \sum^{\infty} \gamma^t r_t$$ starting at some initial state $$s_0$$</p>
  
  <figure><img src="learning.png" style="width:60%; height:60%; text-align:center; display:block; margin-left:auto; margin-right:auto"/></figure>

  <aside>The strong assumption of reinforcement learning is the <b>reward hypothesis</b>, all goals can be described as maximization of some expected cumulative rewards.</aside>
  
  <a class="marker" href="#section-2" id="section-2"><span>2</span></a>
  <h2>Markov Decision Process (MDP)</h2>

  <p> A <b>Markov Decision process</b> is a simpler setup of a reinforcement learning problem. A <b>fully-observable</b> MDP is defined to be a tuple $$(S, A, P, r)$$, where </p>
  <ul>
    <li> $$S$$ is the set of finite states the agent can be in </li>
    <li> $$A$$ is the set of finite actions the agent can take at time step $$t$$ </li>
    <li> $$P(s, a, s') = P(s' | s,  a)$$ is the probability transition matrix of being in state $$s'$$ while in state $$s$$ and taking action $$a$$
    <li> $$r(s, a, s')$$ is the reward of ending up in state $$s'$$ after taking action $$a$$ in state $$s$$
    <li> <b>observable condition</b> both $$P$$ and $$r$$ are visible to the agent </li>
  </ul>

  <p>Notice that the last condition makes this a fully-observable MDP. Any methods used by the agent with these information is called model-based method, as opposed to model-free methods which don't exploit this information.</p>

  <a class="marker" href="#section-3" id="section-3"><span>3</span></a>
  <h2>Bellman equation and exact solution method</h2>
  <p>It should be immediate to the astute readers that with full knowledge of the environment, we can construct a recursive formula describing the <b>dynamics</b> of the system. First, let's define some functions,</p>

  <ul>
    <li> $$\pi$$ is the <b>policy</b> of the agent, where $$\pi(a | s)$$ is the probability distribution over the action space given the current state is $$s$$</li>
    <li> $$G_t = \sum^{\infty} \gamma^t R_t $$, a random variable describing total discounted rewards at time $$t$$ and initial state $$s$$. ($$R_t$$ is the random variable describing reward received at time $$t$$, note that this is different from $$r$$ the reward function)</li>
    <li> $$v_{\pi}(s) = E[ G_t | S_t = s ]$$ is the state-value function describing the expected total discounted rewards of a state, if the agent follows policy $$\pi$$.
    <li> $$q_{\pi}(s, a) = E[G_t| S_t = s, A_t = a ]$$, the action-value function describing the expected total discounted rewards of a state-action pair, if the agent follows policy $$\pi$$.
  </ul>

  <p>A recursive formula, a.k.a Bellman equation, can be immediately derived,</p>

  <d-math block>
    v_{\pi}(S_t) = E[G_t | S_t ] = E[\sum^{\infty} \gamma^t R_t | S_t  ]
                      = E[R_t + \gamma G_{t+1} | S_t ]
                      = E[R_t + \gamma v_{\pi}(S_{t+1}) | S_t]
  </d-math>

  <p>Similarly, for action-value function,</p>

  <d-math block>
    \begin{aligned}
    q_{\pi}(S_t, A_t) &= E[G_t | S_t, A_t] = E[\sum^{\infty} \gamma^t R_t | S_t, A_t] \\
                      &= E[R_t + \gamma G_{t+1} | S_t, A_t] \\
                      &= E[R_t + \gamma Q_{\pi}(S_{t+1}, A_{t+1}) | S_t, A_t] \\
    \end{aligned}
  </d-math>

  <p>The Bellman expectation equation tells us the relationship between the action-value function and the state-value function,</p>

  <d-math block>
    v_{\pi}(s) = E_{\pi}[q_{\pi}(s, a) ] = \sum_{a \in A}  \pi (a | s) q_{\pi}(s, a)
  </d-math>

  and, 
  <d-math block>
    q_{\pi}(s, a) = \sum_{s' \in S} P(s, a, s') (R(s, a, s') + \gamma v_{\pi}(s'))
  </d-math>

  <p>So,</p>
  <d-math block>
    \begin{aligned}
    v_{\pi}(s) &= E_{\pi}[q_{\pi}(s, a) ] \\
    &= \sum_{a \in A}  \pi (a | s) q_{\pi}(s, a) \\
    &=  \sum_{a \in A, s' \in S}  \pi (a | s) P(s, a, s') (R(s, a, s') + \gamma v_{\pi}(s'))
    \end{aligned}
  </d-math>

  <p>This tells us that if we fix the policy $$\pi$$ we can use dynamic programming to work backward from the terminal state and find the value function (and action-value function) for all states and actions. The natural idea is to start with a random policy, then evaluate the value function with the equation above, then improve the new policy by acting greedily with respect to the value function, and repeat until convergence. This is called <b>Policy iteration</b> method. The idea can be demonstrated in the picture below,</p>

  <figure><img src="policy_iter.png"/></figure>

  <p>Similarly, if we know the optimal value function for all the subproblems $$v_{*}(s')$$, then we can find $$v_{*}(s)$$ by doing one-step look ahead, </p>
  
  <d-math block>
  </d-math>
  
  <p>Therefore, we can also iterate directly on the value function. This is called the <b>Value iteration</b> method.</p>

  <a class="marker" href="#section-3" id="section-3"><span>3</span></a>
  <h2>Implementation</h2>

  <p>The following code assumes we have access to <a href="https://github.com/yandexdataschool/Practical_RL/blob/master/week2_value_based/mdp.py"> MDP class</a>, all credit goes to Yandex school of datascience. First, let's implement <b>Value Iteration</b></p>

  <h3>Value Iteration</h3>
  <p>Let's recall the equations for value iteration</p>
  <d-math block>
    \begin{aligned}
    q_{*}(s, a) &= \sum_{s' \in S} P(s, a, s') (R(s, a, s') + \gamma v_{*}(s')) \\
    v_{*}(s) &= max_{a \in A} q_{*}(s, a) \\
  \end{aligned}
  </d-math>
  <p>The first equation can be written in code as follows,</p>
  <d-code block language="python">
    def get_action_value(mdp, state_values, state, action, gamma):
    """ Computes q(s,a) as in formula above """

    res = 0
    for s_prime in mdp.get_all_states():
        prob = mdp.get_transition_prob(state, action, s_prime)
        reward = mdp.get_reward(state, action, s_prime)
        res += prob * (reward + gamma * state_values[s_prime])

    return res

  </d-code>

  <p>The second equation can be written in the code as follows,</p>
  <d-code block language="python">
    def get_new_state_value(mdp, state_values, state, gamma):
    """ Computes next V(s) as in formula above."""
    if mdp.is_terminal(state):
        return 0
    optimal_action_value = 0
    optimal_action = -1
    for action in mdp.get_possible_actions(state):
        q = get_action_value(mdp, state_values, state, action, gamma)
        if q > optimal_action_value:
            optimal_action_value = q
            optimal_action = action
    return optimal_action_value
  </d-code>

  <p>We can now keep iterating until the state values converge</p>
  <d-code block language="python">
  gamma = 0.9
  state_values = {s : 0 for s in mdp.get_all_states()}

  for i in range(100):
      new_state_values = {state: get_new_state_value(mdp, state_values, state, gamma) 
                          for state in mdp.get_all_states()}
      diff = max(abs(new_state_values[s] - state_values[s]) for s in mdp.get_all_states())
      state_values = new_state_values
      if diff < 0.001:
          print("Terminated")
          break
  </d-code>

  <p>After having the optimal state value function (and state-action function), we can now construct an optimal strategy using, $$ \pi_{*}(s) = argmax_{a} q_{*}(s, a)$$</p>
  <figure><img src="result.png" style="width:50%; height:50%; text-align:center; display:block; margin-left:auto; margin-right:auto"/></figure>

  <h3>Policy Iteration</h3>



</d-article>
<d-appendix>
  <d-bibliography src="bibliography.bib"></d-bibliography>
</d-appendix>

<distill-footer></distill-footer>

</body>
